---
title: "Quickstart for R and the DIDE cluster"
author: "Rich FitzJohn"
date: "2017-05-10"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quickstart for R and the DIDE cluster}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



> Get yourself running R jobs on the cluster in 10 minutes or so.

Assumptions that I make here:

* you are using R

* your task can be represented as running a function on some inputs
  to create an output (a file based output is OK)

* you are working on a network share and have this mounted on your
  computer

* you know what packages your code depends on

* your package dependencies are all on CRAN, and are all available
  in windows binary form.

If any of these do not apply to you, you'll probably need to read
the full vignette.  In any case the full vignette contains a bunch
more information anyway.

## Install a lot of packages

Install the packages using [`drat`](https://cran.rstudio.com/package=drat)

```r
# install.package("drat") # if you don't have it already
drat:::add("mrc-ide")
install.packages("didehpc")
```

Or, somewhat equivalently

```r
source("https://mrc-ide.github.io/didehpc/install")
```

## Describe your computer so we can find things

On windows if you are using a domain machine, you should need only
to select the cluster you want to use

```r
options(didehpc.cluster = "fi--didemrchnb")
```

Otherwise, and on any other platform you'll need to provide your username:

```r
options(didehpc.cluster = "fi--didemrchnb",
        didehpc.username = "yourusername")
```

If you are running Linux we can get both your username and password
from the file you use to mount your network shares (see the main
vignette for details)
```r
options(didehpc.cluster = "fi--didemrchnb",
        didehpc.credentials = "~/.smbcredentials")
```

You can see the default configuration with

```r
didehpc::didehpc_config()
```

```
## <didehpc_config>
##  - cluster: fi--didemrchnb
##  - credentials: ~/.smbcredentials
##  - username: rfitzjoh
##  - build_server: builderhv.dide.ic.ac.uk
##  - template: GeneralNodes
##  - hpctools: FALSE
##  - resource:
##     - parallel: FALSE
##     - count: 1
##     - type: Cores
##  - shares:
##     - home: (local) /home/rich/net/home => //fi--san03/homes/rfitzjoh => Q: (remote)
##     - temp: (local) /home/rich/net/temp => //fi--didef3/tmp => T: (remote)
##  - use_workers: FALSE
##  - use_rrq: FALSE
##  - worker_timeout: 600
##  - rtools: FALSE
##  - r_version: 3.3.2
##  - use_java: FALSE
```

If this is the first time you have run this package, best to try
out the login proceedure with:

```r
didehpc::web_login()
```

because this unroots a number of problems early on.

## Describe your project dependencies so we can recreate that on the cluster

Make a vector of packages that you use in your project:

```r
packages <- c("ape", "MASS")
```

And of files that define functions that you ned to run things:

```r
sources <- "mysources.R"
```

If you had a vector here that would be OK too.  The source file
here is very simple:

```r
make_tree <- function(nspp) {
  message("I am building a tree!")
  ape::rtree(nspp)
}

combine <- function(a, b, c) {
  sprintf("%s: %2.5f", a, b + c)
}
```

Then save this together to form a "context".

```r
ctx <- context::context_save("contexts", packages = packages, sources = sources)
```

If you have no packages or no sources, use `NULL` or omit them in
the call below (which is the default anyway).

The first argument here, `"contexts"` is the name of a directory
that we will use to hold a lot of information about your jobs.  You
don't need (or particularly want) to know what is in here.

## Build a queue, based on this context.

This will prompt you for your password, as it will try and log in.

It also installs windows versions of all packages within the
`contexts` directory -- both packages required to get this whole
system working and then the packages required for your particular
jobs.


```r
obj <- didehpc::queue_didehpc(ctx)
```

```
## Loading context 2dac8c240b25e6eed974d5c4bed2b10b
```

```
## Already logged in
```

```
## [ download  ]  package database
```

```
## [ deps      ]  7 extra: crayon, digest, ids, openssl, R6, storr, uuid
```

```
## [ cross     ]  ape
```

```
## [ cross     ]  context
```

```
## [ cross     ]  crayon
```

```
## [ cross     ]  digest
```

```
## [ cross     ]  ids
```

```
## [ cross     ]  openssl
```

```
## [ cross     ]  R6
```

```
## [ cross     ]  storr
```

```
## [ cross     ]  uuid
```

```
## [ cross     ]  MASS
```

Once you get to this point we're ready to start running things on
the cluster.  Let's fire off a test to make sure that everything works OK:

```r
t <- obj$enqueue(sessionInfo())
```

We can poll the job for a while, which will print a progress bar.
If the job is returned in time, it will return the result of
running the function.  Otherwise it will throw an error.

```r
t$wait(120)
```

```
## R version 3.3.2 (2016-10-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows Server 2012 R2 x64 (build 9600)
##
## locale:
## [1] LC_COLLATE=English_United Kingdom.1252
## [2] LC_CTYPE=English_United Kingdom.1252
## [3] LC_MONETARY=English_United Kingdom.1252
## [4] LC_NUMERIC=C
## [5] LC_TIME=English_United Kingdom.1252
##
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base
##
## other attached packages:
## [1] ape_4.1     MASS_7.3-47
##
## loaded via a namespace (and not attached):
## [1] R6_2.2.0        parallel_3.3.2  context_0.1.1   nlme_3.1-128
## [5] grid_3.3.2      digest_0.6.12   storr_1.1.0     lattice_0.20-34
```

You can use `t$result()` to get the result straight away (throwing
an error if it is not ready) or `t$wait(Inf)` to wait forever.

## Running a single task

This is just using the `enqueue` function as above.  But it also
works with functions defined in files passed in as `sources`; here
the function `make_tree`.

```r
t <- obj$enqueue(make_tree(10))
tree <- t$wait(120)
tree
```

```
##
## Phylogenetic tree with 10 tips and 9 internal nodes.
##
## Tip labels:
## 	t1, t8, t5, t6, t9, t7, ...
##
## Rooted; includes branch lengths.
```

The `t` object has a number of other methods you can use:

```r
t
```

```
## <queuer_task>
##   Public:
##     clone: function (deep = FALSE)
##     context_id: function ()
##     expr: function (locals = FALSE)
##     id: 154bfc41d01fe93afba7cbc65f5e282f
##     initialize: function (id, root, check_exists = TRUE)
##     log: function (parse = TRUE)
##     result: function (allow_incomplete = FALSE)
##     root: context_root
##     status: function ()
##     times: function (unit_elapsed = "secs")
##     wait: function (timeout, time_poll = 0.5, progress = NULL)
```

Get the result from running a task

```r
t$result()
```

```
##
## Phylogenetic tree with 10 tips and 9 internal nodes.
##
## Tip labels:
## 	t1, t8, t5, t6, t9, t7, ...
##
## Rooted; includes branch lengths.
```

Get the status of the task

```r
t$status()
```

```
## [1] "COMPLETE"
```

(might also be "PENDING", "RUNNING" or "ERROR"

Get the original expression:

```r
t$expr()
```

```
## make_tree(10)
```

Find out how long everything took

```r
t$times()
```

```
##                            task_id           submitted             started
## 1 154bfc41d01fe93afba7cbc65f5e282f 2017-05-10 14:13:06 2017-05-10 14:13:07
##              finished waiting running      idle
## 1 2017-05-10 14:13:07 1.11066 0.03125 0.5310295
```

You may see negative numbers for "waiting" as the submitted time is
based on your computer and started/finished are based on the
cluster.

And get the log from running the task

```r
t$log()
```

```
## [ hello     ]  2017-05-10 14:13:07.102
## [ wd        ]  Q:/cluster_testing/20170510/vignette
## [ bootstrap ]
## [ lib       ]  Q:\cluster_testing\20170510\vignette\contexts/lib/windows/3.3
## [ init      ]  2017-05-10 14:13:07.118
## [ hostname  ]  FI--DIDEMRC06
## [ process   ]  6544
## [ version   ]  0.1.1
## [ open:db   ]  rds
## [ context   ]  2dac8c240b25e6eed974d5c4bed2b10b
## [ library   ]  ape, MASS
## [ namespace ]
## [ source    ]  mysources.R
## [ parallel  ]  running as single core job
## [ root      ]  Q:\cluster_testing\20170510\vignette\contexts
## [ context   ]  2dac8c240b25e6eed974d5c4bed2b10b
## [ task      ]  154bfc41d01fe93afba7cbc65f5e282f
## [ expr      ]  make_tree(10)
## [ start     ]  2017-05-10 14:13:07.430
##     I am building a tree!
## [ ok        ]
## [ end       ]  2017-05-10 14:13:07.493
##     Warning messages:
##     1: package 'MASS' was built under R version 3.3.3
##     2: package 'ape' was built under R version 3.3.3
```

There is also a bit of DIDE specific logging that happens before
this point; if the job fails inexplicably the answer may be in:

```r
obj$dide_log(t)
```

```
## generated on host: wpia-dide136.dide.ic.ac.uk
## generated on date: 2017-05-10
## didehpc version: 0.1.1
## context version: 0.1.1
## running on: FI--DIDEMRC06
## mapping Q: -> \\fi--san03\homes\rfitzjoh
## The command completed successfully.
## mapping T: -> \\fi--didef3\tmp
## The command completed successfully.
## working directory: Q:\cluster_testing\20170510\vignette
## this is a single task
## logfile: Q:\cluster_testing\20170510\vignette\contexts\logs\154bfc41d01fe93afba7cbc65f5e282f
## Q:\cluster_testing\20170510\vignette>Rscript "Q:\cluster_testing\20170510\vignette\contexts\bin\task_run" "Q:\cluster_testing\20170510\vignette\contexts" 154bfc41d01fe93afba7cbc65f5e282f  1>"Q:\cluster_testing\20170510\vignette\contexts\logs\154bfc41d01fe93afba7cbc65f5e282f" 2>&1
## Quitting
```

## Running a bunch of tasks

There are two broad options here;

1. Apply a function to each element of a list, similar to `lapply`
with `$lapply`
2. Apply a function to each row of a data.frame perhaps using each
column as a different argument with `$enqueue_bulk`

Suppose we want to make a bunch of trees of different sizes.  This
would involve mapping our `make_tree` function over a vector of
sizes:

```r
sizes <- 3:8
grp <- obj$lapply(sizes, make_tree)
```

```
## Creating bundle: 'undivined_bufeo'
```

```
## submitting 6 tasks
```

By default, `$lapply` returns a "task_bundle" with an
automatically generated name.  You can customise the name with the
`name` argument.

Get the startus of all the jobs

```r
grp$status()
```

```
## cb686913f3009eef31e709aac3ddbdd6 e1a8e7ecbd3a1322cd4ba039589e10bc
##                       "COMPLETE"                       "COMPLETE"
## a0c2e91e7d3fc013b4d9ce4290e24f9a 91c43f9dce04ae792aa80e51d2544462
##                       "COMPLETE"                       "COMPLETE"
## 9fab9d07557fefd8fc97084b1ba5fc14 0d94ebd914dba0b3a086a2457add526b
##                        "RUNNING"                        "PENDING"
```

Wait until they are all complete and get the results

```r
res <- grp$wait(120)
```

The other bulk interface is where you want to run a function over a
combination of parameters.  Use `$enqueue_bulk` here.

```r
pars <- expand.grid(a = letters[1:3], b = runif(2), c = pi,
                    stringsAsFactors = FALSE)
pars
```

```
##   a         b        c
## 1 a 0.8983897 3.141593
## 2 b 0.8983897 3.141593
## 3 c 0.8983897 3.141593
## 4 a 0.9446753 3.141593
## 5 b 0.9446753 3.141593
## 6 c 0.9446753 3.141593
```

```r
grp <- obj$enqueue_bulk(pars, combine, do_call = TRUE)
```

```
## Creating bundle: 'potential_mockingbird'
```

```
## submitting 6 tasks
```

By default this runs

* `combine(a = pars$a[[1]], b = pars$b[[1]], c = pars$c[[1]])`
* `combine(a = pars$a[[2]], b = pars$b[[2]], c = pars$c[[2]])`
* ...
* `combine(a = pars$a[[6]], b = pars$b[[6]], c = pars$c[[6]])`


```r
res <- grp$wait(120)
res
```

```
## [[1]]
## [1] "a: 4.03998"
##
## [[2]]
## [1] "b: 4.03998"
##
## [[3]]
## [1] "c: 4.03998"
##
## [[4]]
## [1] "a: 4.08627"
##
## [[5]]
## [1] "b: 4.08627"
##
## [[6]]
## [1] "c: 4.08627"
```
