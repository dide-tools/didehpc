# didewin

> DIDE Windows Cluster Support

**WARNING: Nothing in here will stay the same; use at your own risk**

**NOTICE**: This will only be of use to people at DIDE, as it uses our [cluster web portal](https://mrcdata.dide.ic.ac.uk/hpc), local cluster, and local network file systems.

# Usage

At this point, only very basic usage is supported.  The idea will be to factor out much of the code in this package into a general clustering package and this one will contain only the driver for our windows cluster.  But for now it is what it is.  As indicated in the warning, the interface here will change!

## General proceedure

Note that at this point the package is still low level (this is meant to go in the middle of two other packages -- one lower level and one nice user interface package) but it should still work OK.

The steps below are described in more detail in the usage section:

1. Ensure that your project is in a directory that the cluster can see (i.e. on one of the network drives).  See [notes](#setting-up-network-shares) at the bottom of this file for instructions
2. Set your DIDE credentials up so that you can log in and tell `didewin` about them.
3. Create a "context" in which future expressions will be evaluated (which will be recreated on the cluster)
4. Create a "queue" that uses that context
5. Queue expressions which will be run at some future time on the cluster
6. Monitor progress, retrieve results, etc.

```{r, echo=FALSE, results="hide"}
knitr::opts_chunk$set(error=FALSE)
```

## Directories

I'm just going to assume your project is self contained and contains no references to absolute paths.  The directory that the calculations need are on a network share.  We can relax some of these requirements in future versions; talk to me.

## Credentials

The simplest way to set these is to set these globally, which can be done most simply by setting and forgetting.  Mine are:

```{r}
didewin::didewin_config_global(credentials="~/.smbcredentials",
                               home="~/net/home",
                               temp="~/net/temp",
                               cluster="fi--didemrchnb")
```

See `?didewin_config` for details here.

Hopefully some of these can be auto-detected at some point.

* `credentials`: The `~/.smbcredentials` file is due to the file system set up I have.  If you put just your username here you will be prompted interactively for your DIDE password.  You can supply a username/password list but that's not ideal because your password ends up in plain text and then in things like your `.Rhistory` file!  On windows this will default to your username.
* `home`: path to the mount point of your home share (not required for windows users)
* `temp`: path to the mount point of your home share (not required for windows users)
* `cluster`: name of the cluster to use.  The other option, and default, is `"fi--dideclusthn"`.

A minimal set up on windows where this can be automated is:

```r
didewin::didewin_config_global(cluster="fi--didemrchnb")
```

Once your credentials are set up, try logging in with:

```{r}
didewin::web_login()
```

which will return an error if login was unsuccessful and otherwise prints nothing.  Your credentials are sent to the server in the same way as with the web portal; base64 encoded (i.e., essentially plain text) over a secure https connection.

In general you will not need to run the `web_login` function above, but it's worth doing so first time to make sure that everything looks OK.

## Contexts

For detailed discussion on this, please see the [`context`](http://dide-tools.github.io/context) package, and especially its [vignette](http://dide-tools.github.io/context/vignettes/context.html).  Essentially "contexts" are recipes for reconstructing your local R environment on a remote computer that might be configured differently to yours.  So, things like:

* installing packages (especially if you run non windows, or have 32/64 bit differences).
* sourcing R scripts to create functions and objects
* global and local environments
* collating variables local to an expression

Once all the above is set up, actually evaluating the R expression you want to run remotely is pretty straightforward.

So, suppose I have a script file `myfunctions.R` that contains simply the function definition:

```{r, echo=FALSE, results="asis"}
cat(c("```r", readLines("myfunctions.R"), "```"), sep="\n")
```

which is just going to create `n` trees each of which have `nspp`.  Perhaps this is some slow tree bootstrapping/mcmc proceedure :)

```{r, echo=FALSE, results="hide"}
loadNamespace("ape") # will be done by context later.
unlink("contexts", recursive=TRUE)
Sys.setenv("CONTEXT_SOURCE_PATH"=normalizePath("~/Documents/Projects/epi/cluster/context"))
```

```{r}
root <- "contexts"
ctx <- context::context_save(packages="ape", sources="myfunctions.R", root=root)
```

This might take a second as part of the process involves downloading context itself so that it can be installed on the cluster.

The important bits here are:

* `packages`: a vector of packages to be loaded via `library` on the cluster
* `sources`: *relative paths* to files to be read via `source`
* `root`: the directory to store all the bits to accomplish this.

The directory `contexts` now contains files:

```{r}
dir(root)
```

but in general you do not need to know or care what is going on in there (see the context package if you're curious).

## Queues

```{r}
obj <- didewin::queue_didewin(ctx)
```

If the above command does not throw an error, then you have successfully logged in.

This is a weird sort of object called an `R6` class.  It's a bit like a Python or Java class if you've come from those languages.

```{r}
obj
```

List tasks that we are running already:

```{r}
obj$tasks_list()
```

(note that this is only tasks that are being run via didewin/context, *not* all your tasks).

## Queue an expression

This is the bit that starts doing some actual work for you.  I want to run the `make_trees` function defined in `myfunctions.R`.

To make 5 trees of 10 species each:

```{r, submit1}
t <- obj$enqueue(make_trees(5, 10))
```

The returned object is another R6 object that can be used to query the task

```{r}
t
```

```{r}
t$status()
t$times()
```

Trying to get a task before it is done is an error:

```{r, error=TRUE}
t$result()
```

However, we can wait until it is done (here, waiting for up to 1000s)

```{r, wait1}
res <- t$wait(1000)
res[[1]]
```

Success!

```{r}
t$status()
t$times()
t$result()[[1]]
```

The rather long running time above (`r t$times()$running` s) is due to downloading and installing all the required packages.  This is a bit clearer in the log for that job (this function can be run even when a task is running still).

```{r}
t$log()
```

Future expressions that use the same packages will be faster:

```{r, submit2}
t2 <- obj$enqueue(make_trees(1, 20))
res2 <- t2$wait(1000)
obj$tasks_times()[c("task_id", "submitted", "running")]
```

In the log you can see a quicker startup period
```{r}
t2$log()
```

The task handles can be recreated from the queue

```{r}
obj$tasks_list()
obj$task_get(obj$tasks_list()[[1]])
```

The queue can be recreated in a fresh R session.  Unfortunately at the moment that's really ugly:

```r
ctx <- context:::context_handle(dir, root(file.path(root, "contexts")))
obj <- didewin::queue(ctx)
```

# Next steps

This is an experiment.  Nicer tools will be built on top of this; things like `lapply` for clusters.

Other directions I am thinking of:

* workflow support for where a very large number of jobs need to be submitted but throttled
* jobs that depend on other jobs
* run part of the queue locally at the same time it's running on the cluster, which could allow ad-hoc clustering.

# Installation

This package requires [context](https://github.com/dide-tools/context) so install that first.

```r
devtools::install_github(c(
  "gaborcsardi/progress",
  "dide-tools/context",
  "richfitz/queuer",
  "dide-tools/didewin"))
```

# Setting up network shares

For all options you will need to be on the wired network or VPN I believe.

## Windows

This is done automatically.  `Q:` is your home directory and `T:` is the temporary directory.

## Linux

Full instructions [on the Ubuntu community wiki](https://help.ubuntu.com/community/MountWindowsSharesPermanently).

Install cifs-utils

```
sudo apt-get install cifs-utils
```

In your `/etc/fstab` file, add

```
//fi--san02/homes/<dide-username> <home-mount-point> cifs uid=<local-userid>,gid=<local-groupid>,credentials=/home/<local-username>/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0
//fi--didef2/tmp <tmp-mount-point> cifs uid=<local-userid>,gid=<local-groupid>,credentials=/home/<local-username>/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0
```

where:

- `<dide-username>` is your dide username without the `DIDE\` bit.
- `<local-username>` is your local username (i.e., `echo $USER`).
- `<local-userid>` is your local numeric user id (i.e. `id -u $USER`)
- `<local-groupid>` is your local numeric group id (i.e. `id -g $USER`)
- `<home-mount-point>` is where you want your DIDE home directory mounted
- `<tmp-mount-point>` is where you want the DIDE temporary directory mounted

**please back this file up before editing**.

So for example, I have:

```
//fi--san02/homes/rfitzjoh /home/rich/net/home cifs uid=1000,gid=1000,credentials=/home/rich/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0
//fi--didef2/tmp /home/rich/net/temp cifs uid=1000,gid=1000,credentials=/home/rich/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0
```

The file `.smbcredentials` contains

```
username=<dide-username>
password=<dide-password>
```

and set this to be chmod 600 for a modicum of security.

This set up is clearly insecure.  I believe if you omit the credentials line you can have the system prompt you for a password interactively, but I'm not sure how that works with automatic mounting.

Finally, run

```
mount -a
```

to mount all drives and with any luck it will all work and you don't have to do this until you get a new computer.

## Mac

Guidance welcomed from anyone who can get this reliably working.

# Non-CRAN packages with compiled code

Packages that include compiled code represent a challenge, as every node on the cluster needs a working windows compiler toolchain and that's tricky to guarantee.  This is even worse if you need C++11 support as the current R/Windows/C++ toolchain does not support C++11, but there is experimental support we can try and use, but that requires rebuilding R itself (and all packages that it uses) so it's not a great solution.

Options:

* Install Rtools on the cluster and require recent R versions for jobs that require it.
* Compile the package on windows and add to `context`'s drat repository:
  - via R-win-builder (automatic submission via `devtools` but no automatic collection).
  - appveyor
  - a dedicated windows build machine

The last option is probably the best for us as it will scale the most easily.  There is a prototype version of this working at `129.31.25.12` (behind the VPN) using [buildr](https://github.com/richfitz/buildr).

# Other things

R resources on the cluster are here: `\\fi--didef2\software\HPC\R`
